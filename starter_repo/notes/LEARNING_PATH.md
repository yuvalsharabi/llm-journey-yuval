# Yuval's LLM Learning Path ğŸš€

## ğŸ¯ Goal
Become an expert in **LLM Engineering** in order to **lead development teams** and deliver production-grade AI solutions.

---

## ğŸ“ Stage 1 â€“ Foundations (Hands-on RAG)
- [x] Set up Python environment + venv.
- [x] Create project structure (`apps/`, `scripts/`, `data/`, `models/`).
- [x] Ingest raw documents â†’ chunk into smaller pieces.
- [x] Build FAISS index (currently with **fake embeddings**).
- [x] Expose FastAPI service (`/health`, `/query`) returning top matching chunks.

âœ… Outcome: You have a **working Retrieval system** (the â€œRâ€ in RAG).

---

## ğŸ“ Stage 2 â€“ Real Embeddings
- [ ] Install `sentence-transformers`.
- [ ] Replace fake embeddings with real embeddings (`all-MiniLM-L6-v2`).
- [ ] Rebuild the index and test retrieval on larger documents.
- [ ] Validate that questions return relevant context chunks.

âœ… Outcome: Retrieval is now **semantic**, not random.

---

## ğŸ“ Stage 3 â€“ Generation (the â€œGâ€ in RAG)
- [ ] Add a text generator model (start with **Flan-T5** locally).
- [ ] Connect generator to retrieved chunks to form full answers.
- [ ] Compare answers with and without retrieval.

âœ… Outcome: End-to-end **RAG pipeline**: docs â†’ chunks â†’ embeddings â†’ retrieval â†’ generated answer.

---

## ğŸ“ Stage 4 â€“ Observability & Metrics
- [ ] Add logging for queries (latency, #tokens, top-k hits).
- [ ] Track retrieval quality (are chunks relevant?).
- [ ] Simple evaluation dataset for Q&A.

âœ… Outcome: Ability to measure and improve system performance.

---

## ğŸ“ Stage 5 â€“ Scale & Production
- [ ] Replace local FAISS with a vector DB (Pinecone, Weaviate, or pgvector).
- [ ] Add guardrails (e.g., max query length, input sanitization).
- [ ] Containerize with Docker.
- [ ] Deploy to AWS (API Gateway + Lambda / ECS).
- [ ] Secure with Cognito (authn/authz).
- [ ] Automate pipeline (ingest â†’ index â†’ deploy).

âœ… Outcome: Production-ready **RAG system**, cloud-deployed, scalable, and secure.

---

## ğŸ“ Stage 6 â€“ Advanced Topics
- Fine-tuning with **PEFT/QLoRA**.
- Knowledge distillation to smaller models.
- Hybrid retrieval (sparse + dense).
- Integrations with Bedrock, SageMaker, or Azure OpenAI.
- Monitoring hallucinations, adding guardrails.

---

## ğŸ“ Notes
- This file is your â€œmapâ€ â€“ update checkboxes as you progress.
- At each stage, ask your GPT assistant:  
  *â€œGuide me to complete the next stage in my Learning Path.â€*
- Keep documenting learnings in `notes/` for future reference.

---
